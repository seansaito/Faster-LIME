{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Explainer in pure Numpy and Scikit-learn\n",
    "\n",
    "## Steps:\n",
    "\n",
    "* Sample data and generate synthetic neighborhood\n",
    "    * For text, this involves removing words from the document\n",
    "    * Use BoW to create a binary vector\n",
    "* Get model predictions\n",
    "    * This involves returning binary data back to data domain\n",
    "    * Split data into tokens defined by CountVectorizer analyzer\n",
    "    * Index based on which non-zero elements of binary vector (for each synthetic example)\n",
    "    * Concatenate to form \"raw text\" (default: whitespace)\n",
    "* Solve\n",
    "* Explain (i.e. get the most important features)\n",
    "\n",
    "## Notes on how LIME does it\n",
    "\n",
    "* `lime.lime_text.__data_labels_distances`\n",
    "* Sample N random integers, each integer representing how many words to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 20newsgroups and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some auxiliary imports for the tutorial\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'filenames', 'target_names', 'target', 'DESCR']\n",
      "['rec.sport.baseball', 'sci.med', 'soc.religion.christian']\n",
      "[1 0 2 2 0 2 0 0 0 1]\n",
      "(1790, 29009)\n",
      "(1191, 29009)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9689336691855583"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on a subset of categories\n",
    "\n",
    "categories = [\n",
    "    'rec.sport.baseball',\n",
    "    'soc.religion.christian',\n",
    "    'sci.med'\n",
    "]\n",
    "\n",
    "raw_train = datasets.fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(list(raw_train.keys()))\n",
    "print(raw_train.target_names)\n",
    "print(raw_train.target[:10])\n",
    "raw_test = datasets.fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(raw_train.data)\n",
    "y_train = raw_train.target\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "X_test = vectorizer.transform(raw_test.data)\n",
    "y_test = raw_test.target\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1790x29009 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 291648 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: luriem@alleg.edu(Michael Lurie) The Liberalizer\\nSubject: Re: RE:Re:ALL-TIME BEST PLAYERS\\nOrganization: Allegheny College\\nLines: 20\\n\\nIn article <1993Apr21.120525.1@tesla.njit.edu> drm6640@tesla.njit.edu  \\nwrites:\\n> Overall (career)\\n> 1.\\tDon Mattingly\\n> 2.\\tDon Mattingly\\n> 3.\\tDon Mattingly\\n> 4.\\tDon Mattingly\\n> 5.\\tDon Mattingly\\n> 6.\\tDon Mattingly\\n> 7.\\tDon Mattingly\\n> 8.\\tDon Mattingly\\n> 9.\\tDon Mattingly\\n> 10.\\tDon Mattingly\\n> 11.\\tDon Mattingly\\n> ..\\n\\n\\nWanna go to a game sometime?\\nJesus christ boy, have you not heard of the real all-time best....STEVE  \\nBALBONI...Now that's Yankee pride.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(instance):\n",
    "    vec = vectorizer.transform(instance)\n",
    "    return clf.predict_proba(vec)\n",
    "\n",
    "\n",
    "lime_text_explainer = LimeTextExplainer(\n",
    "    class_names=categories\n",
    ")\n",
    "\n",
    "exp = lime_text_explainer.explain_instance(\n",
    "    text_instance=raw_test.data[0],\n",
    "    classifier_fn=predict_fn,\n",
    "    labels=(0, 1, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mattingly', 0.1630204569197587),\n",
       " ('Yankee', 0.047128435532711455),\n",
       " ('Lurie', 0.04592710278967293),\n",
       " ('PLAYERS', 0.045541508852427214),\n",
       " ('Allegheny', 0.04400147104174964),\n",
       " ('luriem', 0.04385267215867704),\n",
       " ('Liberalizer', 0.04244576588487228),\n",
       " ('Don', -0.030393475108189762),\n",
       " ('tesla', -0.04552783302602691),\n",
       " ('njit', -0.05400846560032084)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(exp.as_list(0), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(raw_test.data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = count_vectorizer.transform(raw_test.data[:1]).toarray()\n",
    "vec / vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['10', '11', '120525', '1993apr21', '20', 'all', 'alleg',\n",
       "        'allegheny', 'article', 'balboni', 'best', 'boy', 'career',\n",
       "        'christ', 'college', 'don', 'drm6640', 'edu', 'from', 'game', 'go',\n",
       "        'have', 'heard', 'in', 'jesus', 'liberalizer', 'lines', 'lurie',\n",
       "        'luriem', 'mattingly', 'michael', 'njit', 'not', 'now', 'of',\n",
       "        'organization', 'overall', 'players', 'pride', 're', 'real',\n",
       "        'sometime', 'steve', 'subject', 'tesla', 'that', 'the', 'time',\n",
       "        'to', 'wanna', 'writes', 'yankee', 'you'], dtype='<U12')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.inverse_transform(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_char_ngrams', '_char_wb_ngrams', '_check_stop_words_consistency', '_check_vocabulary', '_count_vocab', '_get_param_names', '_get_tags', '_limit_features', '_more_tags', '_sort_features', '_stop_words_id', '_validate_custom_analyzer', '_validate_params', '_validate_vocabulary', '_warn_for_unused_params', '_white_spaces', '_word_ngrams', 'analyzer', 'binary', 'build_analyzer', 'build_preprocessor', 'build_tokenizer', 'decode', 'decode_error', 'dtype', 'encoding', 'fit', 'fit_transform', 'fixed_vocabulary_', 'get_feature_names', 'get_params', 'get_stop_words', 'input', 'inverse_transform', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'preprocessor', 'set_params', 'stop_words', 'stop_words_', 'strip_accents', 'token_pattern', 'tokenizer', 'transform', 'vocabulary', 'vocabulary_']\n"
     ]
    }
   ],
   "source": [
    "print(dir(count_vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function _analyze at 0x7fc961b9d320>, ngrams=<bound method _VectorizerMixin._word_ngrams of CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)>, tokenizer=<built-in method findall of re.Pattern object at 0x7fc95f1719f0>, preprocessor=functools.partial(<function _preprocess at 0x7fc961bd1d40>, accent_function=None, lower=True), decoder=<bound method _VectorizerMixin.decode of CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)>, stop_words=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = count_vectorizer.build_analyzer()\n",
    "analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'luriem', 'alleg', 'edu', 'michael', 'lurie', 'the', 'liberalizer', 'subject', 're', 're', 're', 'all', 'time', 'best', 'players', 'organization', 'allegheny', 'college', 'lines', '20', 'in', 'article', '1993apr21', '120525', 'tesla', 'njit', 'edu', 'drm6640', 'tesla', 'njit', 'edu', 'writes', 'overall', 'career', 'don', 'mattingly', 'don', 'mattingly', 'don', 'mattingly', 'don', 'mattingly', 'don', 'mattingly', 'don', 'mattingly', 'don', 'mattingly', 'don', 'mattingly', 'don', 'mattingly', '10', 'don', 'mattingly', '11', 'don', 'mattingly', 'wanna', 'go', 'to', 'game', 'sometime', 'jesus', 'christ', 'boy', 'have', 'you', 'not', 'heard', 'of', 'the', 'real', 'all', 'time', 'best', 'steve', 'balboni', 'now', 'that', 'yankee', 'pride']\n"
     ]
    }
   ],
   "source": [
    "print(analyzer(raw_test.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '11', '120525', '1993apr21', '20', 'all', 'alleg', 'allegheny', 'article', 'balboni', 'best', 'boy', 'career', 'christ', 'college', 'don', 'drm6640', 'edu', 'from', 'game', 'go', 'have', 'heard', 'in', 'jesus', 'liberalizer', 'lines', 'lurie', 'luriem', 'mattingly', 'michael', 'njit', 'not', 'now', 'of', 'organization', 'overall', 'players', 'pride', 're', 'real', 'sometime', 'steve', 'subject', 'tesla', 'that', 'the', 'time', 'to', 'wanna', 'writes', 'yankee', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a particular document, we want to map the token to its index in the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'from': [0],\n",
       "             'luriem': [1],\n",
       "             'alleg': [2],\n",
       "             'edu': [3, 27, 31],\n",
       "             'michael': [4],\n",
       "             'lurie': [5],\n",
       "             'the': [6, 72],\n",
       "             'liberalizer': [7],\n",
       "             'subject': [8],\n",
       "             're': [9, 10, 11],\n",
       "             'all': [12, 74],\n",
       "             'time': [13, 75],\n",
       "             'best': [14, 76],\n",
       "             'players': [15],\n",
       "             'organization': [16],\n",
       "             'allegheny': [17],\n",
       "             'college': [18],\n",
       "             'lines': [19],\n",
       "             '20': [20],\n",
       "             'in': [21],\n",
       "             'article': [22],\n",
       "             '1993apr21': [23],\n",
       "             '120525': [24],\n",
       "             'tesla': [25, 29],\n",
       "             'njit': [26, 30],\n",
       "             'drm6640': [28],\n",
       "             'writes': [32],\n",
       "             'overall': [33],\n",
       "             'career': [34],\n",
       "             'don': [35, 37, 39, 41, 43, 45, 47, 49, 51, 54, 57],\n",
       "             'mattingly': [36, 38, 40, 42, 44, 46, 48, 50, 52, 55, 58],\n",
       "             '10': [53],\n",
       "             '11': [56],\n",
       "             'wanna': [59],\n",
       "             'go': [60],\n",
       "             'to': [61],\n",
       "             'game': [62],\n",
       "             'sometime': [63],\n",
       "             'jesus': [64],\n",
       "             'christ': [65],\n",
       "             'boy': [66],\n",
       "             'have': [67],\n",
       "             'you': [68],\n",
       "             'not': [69],\n",
       "             'heard': [70],\n",
       "             'of': [71],\n",
       "             'real': [73],\n",
       "             'steve': [77],\n",
       "             'balboni': [78],\n",
       "             'now': [79],\n",
       "             'that': [80],\n",
       "             'yankee': [81],\n",
       "             'pride': [82]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "analyzed_text = count_vectorizer.build_analyzer()(raw_test.data[0])\n",
    "dict_token_idxes = defaultdict(list)\n",
    "dict_idx_to_token = dict()\n",
    "for (idx, key) in enumerate(analyzed_text):\n",
    "    dict_token_idxes[key].append(idx)\n",
    "    dict_idx_to_token[idx] = key\n",
    "    \n",
    "dict_token_idxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now sampling: we randomly draw out 1's from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 53)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = count_vectorizer.transform(raw_test.data[:1]).toarray()\n",
    "vec = vec / vec\n",
    "num_samples = 1000\n",
    "dim = vec.shape[1]\n",
    "\n",
    "mask = np.ones((num_samples, dim))\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 33, 10, 14, 39, 34, 38, 22,  4, 17, 48, 35, 13, 45, 36, 45, 12,\n",
       "       35, 28, 15, 39, 33, 12, 10, 10,  5,  1, 15, 49, 11, 46, 14,  3, 27,\n",
       "        9, 52, 22, 26,  2, 18, 29, 37, 50, 15, 42, 47, 17, 20, 44, 39, 23,\n",
       "        3, 50, 36,  9, 32, 22, 17, 26,  3, 40,  1, 32, 48, 21, 45, 27, 20,\n",
       "       39, 48, 20,  4,  2,  9,  8,  6, 20, 49, 38, 39,  6, 15, 23,  5, 23,\n",
       "        1, 29, 12, 32, 44,  9, 36, 11, 25, 14, 40, 38, 42,  6, 33,  1, 11,\n",
       "       21, 30, 24, 25, 38, 33, 32,  3, 39, 23, 24,  1, 12, 43, 25, 18, 17,\n",
       "       37,  7,  6, 31, 49, 21, 20, 49, 38, 44, 42, 10, 24, 47, 41,  8, 20,\n",
       "       24, 13, 15, 12, 30, 31, 28, 31, 16, 19,  7, 45, 33,  9, 28,  5, 10,\n",
       "        6,  7, 41, 23, 40, 43, 10, 39, 19, 35, 42, 35, 41, 43, 45,  1, 43,\n",
       "       19, 20, 30, 22, 14, 38, 24, 12, 36, 40, 36, 15,  4,  5, 37, 20, 11,\n",
       "       22, 29, 28,  2, 22, 40,  4,  5, 30, 45,  9, 28,  9, 22, 23,  2, 44,\n",
       "       50, 24, 48, 11, 12, 10, 10, 33,  6, 29, 14, 40, 30, 37, 13,  2, 23,\n",
       "       47, 48,  9, 45, 46, 25, 10, 11,  5, 34, 43, 41, 22, 14, 13, 37, 48,\n",
       "       36, 39, 39,  1, 26,  5, 29, 40, 13, 16, 35, 20,  7, 15, 36, 38, 24,\n",
       "        1, 18, 19, 33,  1, 41,  6, 10, 44, 26, 16, 35, 29, 43, 17, 46, 45,\n",
       "        3, 15, 29, 42, 41,  5, 52, 18, 37, 37,  6, 43,  1, 33, 35, 22, 40,\n",
       "       35, 22, 33, 12, 13,  3,  3, 28, 44, 20, 17, 26, 33,  7, 43, 28, 19,\n",
       "       12, 21, 29,  7, 28, 20, 17, 30,  1,  4, 40, 41, 16, 38, 16,  7, 16,\n",
       "       21, 39, 49, 31, 38, 34,  1,  5,  3, 12, 51, 30, 31, 50, 40, 17, 14,\n",
       "       36, 48, 26, 35, 24, 15, 11,  4, 11, 25, 42, 44, 22, 39, 32, 31, 15,\n",
       "       30,  4, 17, 43, 17, 26, 11, 11,  7, 29, 48, 42, 43,  2,  2, 25, 22,\n",
       "       48, 43, 17, 34, 42,  6, 34, 42, 10, 42, 24,  8, 33,  7, 42, 33, 27,\n",
       "       28,  5, 52,  6, 25, 36, 18,  3, 10, 24, 33, 51, 33, 31,  2, 36, 34,\n",
       "       27, 41, 27, 29, 30,  4, 50,  4, 37, 51,  6,  9, 46, 11, 11, 15, 15,\n",
       "       28,  1, 14, 36,  2, 30, 16, 30, 20,  4, 44, 21, 36, 47, 43, 44, 39,\n",
       "       30,  1, 30, 21, 47, 43,  2, 34, 27, 27, 39,  9, 45, 11, 50,  2,  5,\n",
       "       48, 16, 49, 28, 20, 42, 20, 47, 45, 42, 37, 20, 27, 52, 31, 22, 13,\n",
       "       15, 20, 47, 15, 14, 41, 21, 18, 18, 41, 47, 21, 50, 36, 36, 42, 42,\n",
       "        4, 22, 19, 31, 45,  2, 27, 45, 46, 36,  5, 25, 13, 24, 31, 52, 22,\n",
       "       50, 26, 48,  7, 45, 30, 49,  8, 37, 30, 10, 27, 47, 50, 18, 41, 33,\n",
       "       45, 29,  8, 52, 39, 34, 48, 51, 38, 50, 41, 45, 37, 30, 45, 30,  8,\n",
       "       28, 40, 22, 32, 12, 34, 35, 20, 24, 52, 23,  3,  5, 31, 32, 37, 29,\n",
       "       32, 17, 38, 52,  2,  5, 52, 52, 50, 30,  2, 32, 25, 28, 52, 21, 49,\n",
       "        5, 19, 12,  6,  8, 39, 25, 47, 48, 46,  8,  6,  8, 32,  8,  2, 21,\n",
       "       26, 34, 48, 14,  6, 17,  4,  7, 29, 31,  7, 16,  8, 26,  2, 45,  4,\n",
       "       51, 29, 14, 18, 46, 10, 13, 34, 24, 39, 23, 44, 33, 23, 29, 43, 20,\n",
       "       20,  5, 38,  5, 42, 34, 20, 26,  1, 17,  9,  7, 17, 45, 31, 16, 25,\n",
       "        7,  3, 22, 36, 38, 50, 22,  3, 16, 42, 11, 13,  4, 11, 17, 35, 41,\n",
       "       38, 34, 23,  5,  1, 49, 18, 52,  5, 46, 40, 41, 45, 24, 48, 30, 34,\n",
       "       27, 44, 28, 36, 26, 31, 25, 50,  4, 40, 22, 13, 22, 35, 36, 30,  5,\n",
       "       36, 24,  3, 19, 51, 45, 30, 48, 48, 50,  5, 29, 26, 23, 41, 25, 14,\n",
       "       33,  6, 33, 14,  3,  1, 44, 37, 28, 43,  8, 27, 36, 51, 24, 24, 14,\n",
       "       40, 51, 42, 20, 10, 25, 32, 48,  1, 32, 14,  2, 11, 28, 38, 44, 18,\n",
       "       14, 15, 34, 33, 45, 24, 50, 12, 13, 41, 16, 30, 30, 37, 15, 19,  2,\n",
       "       18, 28, 16, 16, 49, 10, 21,  4, 43, 46, 14, 10, 46,  6, 41, 37, 39,\n",
       "       44, 15,  1, 23, 51, 13, 22, 28,  2, 50, 36,  2,  2, 25, 45, 23, 48,\n",
       "        8, 13,  8, 14, 45, 39, 23, 42, 16,  4, 29, 37, 33,  2, 20,  1, 32,\n",
       "       13, 36,  1, 38, 48, 31, 13, 48, 30, 49, 23, 31, 39, 48,  9, 33,  4,\n",
       "       44, 11, 28, 40, 19, 38, 14, 33, 38,  1, 38,  5, 25, 17, 30,  2,  2,\n",
       "       29, 15,  2, 41, 17, 10, 18, 36,  1, 13, 15, 31, 22, 51, 40, 16, 38,\n",
       "       49, 34, 22,  6, 27, 34, 24, 49, 48,  8, 13, 43, 36, 13,  3, 13, 17,\n",
       "       47, 46, 37, 24, 35, 32, 28, 35, 36, 50,  4,  9, 43, 11,  8,  2,  3,\n",
       "       14, 29, 33, 43, 40, 12, 37, 28, 11, 49, 13, 17, 12, 33, 30, 10, 17,\n",
       "       46, 36, 34, 24, 28, 17, 21, 41, 28, 36, 13, 15, 17,  7, 12, 36, 46,\n",
       "        5, 27, 44,  2, 37,  1, 40,  5, 48, 51,  8, 28, 18,  7, 48, 33, 31,\n",
       "       42,  6, 26, 43, 32, 28, 43, 36, 24,  7, 39,  3,  1,  4, 41, 44, 23,\n",
       "       43, 36, 43, 46, 22, 12, 37, 16, 50, 49, 23,  2, 29,  5, 42, 43, 23,\n",
       "       39, 16, 17,  9, 31, 38, 45,  8, 30,  2, 23, 50, 35,  4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_mask = np.random.randint(1, dim, size=num_samples)\n",
    "num_words_mask[0] = dim\n",
    "num_words_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(x):\n",
    "    return np.random.binomial(1, x/dim, size=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [0, 1, 1, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array(list(map(create_mask, num_words_mask)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 53)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 30, 14, 16, 42, 33, 39, 20,  2, 18, 49, 33,  9, 45, 43, 48, 11,\n",
       "       35, 24, 14, 39, 32,  8,  6, 10,  7,  0, 14, 49, 10, 44, 13,  2, 18,\n",
       "        8, 53, 20, 28,  2, 22, 30, 36, 51, 14, 37, 46, 14, 19, 46, 38, 26,\n",
       "        7, 50, 41, 11, 29, 19, 16, 29,  7, 38,  3, 32, 47, 24, 45, 23, 21,\n",
       "       42, 51, 19,  3,  7, 12, 11,  8, 18, 48, 40, 38,  5, 15, 23,  9, 20,\n",
       "        2, 29, 17, 34, 46,  8, 36, 11, 15, 10, 43, 37, 44,  7, 29,  1, 18,\n",
       "       22, 31, 29, 22, 37, 33, 39,  4, 36, 30, 22,  0,  9, 44, 23, 21, 20,\n",
       "       34, 11,  6, 32, 49, 18, 26, 47, 40, 41, 46, 14, 22, 50, 37,  7, 20,\n",
       "       32, 12,  9, 10, 33, 29, 24, 25, 18, 18,  6, 48, 32, 11, 26,  6, 10,\n",
       "        7, 12, 43, 22, 43, 44,  9, 40, 25, 34, 48, 36, 43, 45, 43,  1, 46,\n",
       "       17, 20, 31, 22, 12, 41, 23, 12, 32, 41, 31, 17,  3,  5, 36, 17, 21,\n",
       "       18, 24, 29,  0, 26, 35,  2,  5, 21, 47, 12, 29,  6, 22, 29,  0, 48,\n",
       "       49, 22, 49, 13, 14,  8, 12, 34,  8, 33, 12, 38, 33, 36, 12,  3, 25,\n",
       "       49, 50,  9, 47, 42, 20, 12,  7,  3, 36, 45, 42, 29, 14,  6, 39, 50,\n",
       "       30, 43, 38,  2, 28,  6, 30, 39, 14, 12, 28, 20, 11, 18, 38, 41, 19,\n",
       "        0, 15, 15, 30,  1, 44, 10,  7, 45, 30, 14, 35, 31, 42, 17, 46, 48,\n",
       "        3, 14, 27, 41, 43, 10, 52, 23, 37, 41,  7, 45,  0, 30, 29, 19, 37,\n",
       "       33, 26, 34, 14, 13,  3,  4, 28, 49, 25, 21, 25, 34,  4, 42, 25, 12,\n",
       "       16, 18, 26,  9, 30, 22, 14, 33,  3,  7, 38, 44, 16, 37, 15,  5, 13,\n",
       "       21, 39, 52, 26, 39, 40,  0,  5,  2, 16, 50, 34, 26, 49, 44, 11, 10,\n",
       "       35, 51, 28, 37, 26, 19,  9,  4, 10, 32, 43, 46, 24, 41, 33, 32, 15,\n",
       "       25,  3, 21, 42, 24, 24, 12, 17,  8, 29, 47, 39, 42,  2,  2, 28, 20,\n",
       "       46, 38, 22, 30, 45,  7, 36, 46,  8, 47, 21, 11, 32, 10, 42, 33, 25,\n",
       "       26,  8, 52,  5, 21, 34, 14,  3,  7, 23, 28, 51, 37, 23,  0, 36, 33,\n",
       "       35, 37, 26, 35, 33,  1, 51,  2, 38, 51,  4,  8, 49, 13, 13, 13, 16,\n",
       "       32,  1, 11, 37,  2, 31, 14, 28, 25,  8, 45, 24, 42, 48, 41, 45, 39,\n",
       "       31,  2, 27, 25, 47, 43,  3, 30, 19, 29, 39, 11, 48, 11, 50,  3,  4,\n",
       "       47, 12, 47, 30, 22, 43, 19, 50, 46, 45, 39, 18, 27, 53, 28, 21, 13,\n",
       "       12, 25, 48, 23, 11, 41, 15, 20, 24, 43, 43, 24, 51, 34, 36, 41, 39,\n",
       "        2, 21, 19, 32, 46,  3, 29, 47, 47, 40,  1, 28, 12, 25, 32, 52, 22,\n",
       "       49, 32, 47, 12, 47, 23, 49, 10, 32, 26,  8, 25, 50, 48, 13, 45, 30,\n",
       "       40, 31,  7, 52, 39, 34, 47, 52, 37, 50, 45, 48, 34, 33, 44, 22,  8,\n",
       "       25, 40, 26, 30, 15, 33, 33, 20, 24, 53, 22,  2,  3, 30, 28, 43, 30,\n",
       "       38, 20, 34, 52,  2,  4, 51, 50, 52, 31,  2, 30, 27, 22, 52, 17, 52,\n",
       "        5, 16, 11,  7, 10, 41, 25, 45, 49, 48,  8,  9,  6, 28, 11,  1, 23,\n",
       "       31, 31, 48, 22,  3, 20,  3,  7, 32, 32, 13, 18,  6, 25,  0, 48,  5,\n",
       "       52, 34, 11, 19, 48,  8, 12, 35, 18, 38, 22, 39, 32, 23, 33, 42, 20,\n",
       "       16,  6, 40,  5, 44, 30, 22, 28,  0, 15,  8,  3, 17, 44, 38, 17, 30,\n",
       "        9,  4, 25, 28, 38, 50, 22,  1, 15, 42,  6, 11,  4, 12, 21, 36, 39,\n",
       "       36, 36, 21,  6,  0, 45, 18, 50,  4, 43, 41, 39, 42, 20, 47, 24, 32,\n",
       "       31, 44, 28, 34, 25, 38, 24, 48,  5, 36, 20, 13, 24, 35, 34, 28,  3,\n",
       "       38, 23,  4, 23, 52, 40, 34, 48, 50, 51,  3, 30, 24, 26, 40, 22, 17,\n",
       "       36,  8, 37, 15,  4,  0, 39, 39, 27, 40,  8, 22, 38, 52, 24, 18, 15,\n",
       "       37, 50, 43, 19, 12, 22, 29, 50,  0, 35, 12,  2,  8, 30, 32, 48, 22,\n",
       "       11, 14, 30, 36, 47, 19, 48,  9, 12, 44, 12, 31, 29, 38, 22, 18,  2,\n",
       "       20, 33, 22, 16, 50, 13, 24,  2, 42, 36, 20, 11, 48,  6, 40, 34, 40,\n",
       "       38, 13,  2, 23, 51, 14, 23, 27,  0, 50, 36,  3,  2, 24, 45, 22, 48,\n",
       "        6, 17,  6, 17, 51, 43, 28, 42, 19,  4, 32, 32, 36,  2, 15,  2, 28,\n",
       "       11, 43,  0, 39, 49, 41, 14, 49, 30, 50, 27, 30, 40, 47,  7, 34,  0,\n",
       "       41, 12, 31, 39, 23, 40,  7, 38, 38,  3, 39,  4, 24, 15, 30,  1,  1,\n",
       "       36, 16,  3, 42, 14,  9, 17, 38,  0, 16, 13, 34, 20, 49, 46, 12, 37,\n",
       "       49, 35, 21,  5, 29, 37, 31, 46, 49,  4,  8, 45, 33, 15,  1, 15, 16,\n",
       "       48, 47, 45, 20, 35, 33, 29, 37, 40, 50,  8, 10, 40, 10, 11,  3,  3,\n",
       "       13, 34, 37, 41, 41, 11, 42, 25, 13, 44, 15, 21, 13, 32, 35,  6, 15,\n",
       "       47, 37, 37, 19, 27, 16, 19, 35, 35, 39,  9, 12, 17,  7, 15, 33, 43,\n",
       "        3, 30, 41,  2, 34,  1, 38,  6, 48, 50,  9, 33, 16,  6, 48, 30, 33,\n",
       "       37,  6, 31, 46, 31, 24, 41, 32, 15,  5, 38,  5,  1,  5, 42, 46, 22,\n",
       "       42, 28, 42, 47, 18,  9, 36, 13, 48, 48, 24,  1, 27,  7, 46, 47, 28,\n",
       "       40, 15, 18,  6, 30, 35, 44, 10, 30,  2, 23, 48, 39,  4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create the synthetic raw text (original document) based on mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '120525', '1993apr21', '20', 'all', 'alleg',\n",
       "       'allegheny', 'article', 'balboni', 'best', 'boy', 'career',\n",
       "       'christ', 'college', 'don', 'drm6640', 'edu', 'from', 'game', 'go',\n",
       "       'have', 'heard', 'in', 'jesus', 'liberalizer', 'lines', 'lurie',\n",
       "       'luriem', 'mattingly', 'michael', 'njit', 'not', 'now', 'of',\n",
       "       'organization', 'overall', 'players', 'pride', 're', 'real',\n",
       "       'sometime', 'steve', 'subject', 'tesla', 'that', 'the', 'time',\n",
       "       'to', 'wanna', 'writes', 'yankee', 'you'], dtype='<U12')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_arr = np.array(count_vectorizer.get_feature_names())\n",
    "word_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '120525', '1993apr21', '20', 'all', 'alleg',\n",
       "       'allegheny', 'article', 'balboni', 'best', 'boy', 'career',\n",
       "       'christ', 'college', 'don', 'drm6640', 'edu', 'from', 'game', 'go',\n",
       "       'have', 'heard', 'in', 'jesus', 'liberalizer', 'lines', 'lurie',\n",
       "       'luriem', 'mattingly', 'michael', 'njit', 'not', 'now', 'of',\n",
       "       'organization', 'overall', 'players', 'pride', 're', 'real',\n",
       "       'sometime', 'steve', 'subject', 'tesla', 'that', 'the', 'time',\n",
       "       'to', 'wanna', 'writes', 'yankee', 'you'], dtype='<U12')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_arr[mask.astype(bool)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [False,  True,  True, ..., False, False,  True],\n",
       "       [ True, False, False, ..., False, False,  True],\n",
       "       ...,\n",
       "       [ True, False,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ..., False, False,  True],\n",
       "       [False, False, False, ..., False,  True, False]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_bool = mask.astype(bool)\n",
    "mask_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['10', '11', '120525', '1993apr21', '20', 'all', 'alleg',\n",
       "        'allegheny', 'article', 'balboni', 'best', 'boy', 'career',\n",
       "        'christ', 'college', 'don', 'drm6640', 'edu', 'from', 'game', 'go',\n",
       "        'have', 'heard', 'in', 'jesus', 'liberalizer', 'lines', 'lurie',\n",
       "        'luriem', 'mattingly', 'michael', 'njit', 'not', 'now', 'of',\n",
       "        'organization', 'overall', 'players', 'pride', 're', 'real',\n",
       "        'sometime', 'steve', 'subject', 'tesla', 'that', 'the', 'time',\n",
       "        'to', 'wanna', 'writes', 'yankee', 'you'], dtype='<U12'),\n",
       " array(['11', '120525', '1993apr21', 'all', 'article', 'balboni', 'boy',\n",
       "        'don', 'game', 'heard', 'in', 'liberalizer', 'lurie', 'mattingly',\n",
       "        'michael', 'njit', 'not', 'now', 'of', 'organization', 'overall',\n",
       "        'players', 'real', 'steve', 'subject', 'tesla', 'that', 'time',\n",
       "        'to', 'you'], dtype='<U12')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_words = [word_arr[mask_bool_row] for mask_bool_row in mask_bool]\n",
    "list_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from' 'luriem' 'alleg' 'edu' 'michael' 'lurie' 'the' 'liberalizer'\n",
      " 'subject' 're' 're' 're' 'all' 'time' 'best' 'players' 'organization'\n",
      " 'allegheny' 'college' 'lines' '20' 'in' 'article' '1993apr21' '120525'\n",
      " 'tesla' 'njit' 'edu' 'drm6640' 'tesla' 'njit' 'edu' 'writes' 'overall'\n",
      " 'career' 'don' 'mattingly' 'don' 'mattingly' 'don' 'mattingly' 'don'\n",
      " 'mattingly' 'don' 'mattingly' 'don' 'mattingly' 'don' 'mattingly' 'don'\n",
      " 'mattingly' 'don' 'mattingly' '10' 'don' 'mattingly' '11' 'don'\n",
      " 'mattingly' 'wanna' 'go' 'to' 'game' 'sometime' 'jesus' 'christ' 'boy'\n",
      " 'have' 'you' 'not' 'heard' 'of' 'the' 'real' 'all' 'time' 'best' 'steve'\n",
      " 'balboni' 'now' 'that' 'yankee' 'pride']\n"
     ]
    }
   ],
   "source": [
    "analyzed_text = np.array(analyzed_text)\n",
    "print(analyzed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn list_word into a list of indices\n",
    "list_raw_synthetic = []\n",
    "for list_word in list_words:\n",
    "    a = []\n",
    "    for word in list_word:\n",
    "        a.extend(dict_token_idxes[word])\n",
    "    a = sorted(a)\n",
    "    list_raw_synthetic.append(' '.join([dict_idx_to_token[idx] for idx in a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edu michael the 20 1993apr21 120525 njit edu njit edu 10 11 game jesus christ boy have heard of the'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_raw_synthetic[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred = predict_fn(list_raw_synthetic)\n",
    "model_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get distances between original sample and synthetic ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 53)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = pairwise_distances(mask[0].reshape((1, -1)), mask, metric='cosine').ravel()\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kernel_fn(distances, kernel_width):\n",
    "    return np.sqrt(np.exp(-(distances ** 2) / kernel_width ** 2))\n",
    "\n",
    "weights = kernel_fn(distances, kernel_width=0.75 * np.sqrt(dim))\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
       "      random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 0\n",
    "num_features = 10\n",
    "solver = Ridge(alpha=1, fit_intercept=True)\n",
    "solver.fit(mask, model_pred[:, label], sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = solver.coef_\n",
    "explanations = sorted(list(zip(word_arr, importances)), \n",
    "                      key=lambda x: x[1], reverse=True)[:num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mattingly', 0.20229552944903878),\n",
       " ('yankee', 0.07616704150331953),\n",
       " ('luriem', 0.07327664229296545),\n",
       " ('players', 0.07302627352955146),\n",
       " ('liberalizer', 0.07155918387960071),\n",
       " ('allegheny', 0.0707025581364165),\n",
       " ('lurie', 0.06929677019260583),\n",
       " ('game', 0.05867673442091009),\n",
       " ('alleg', 0.05447759700085077),\n",
       " ('career', 0.049264448378751505)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(text_instance, predict_fn, label, num_samples=5000, num_features=10):\n",
    "    if type(text_instance) is str:\n",
    "        text_instance = [text_instance]\n",
    "    \n",
    "    # Use Count Vectorizer to vectorize text as BoW\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    bow_vec = count_vectorizer.fit_transform(text_instance).toarray()\n",
    "    bow_vec = bow_vec / bow_vec\n",
    "    word_dim = bow_vec.shape[1]\n",
    "    \n",
    "    # Build word analyzer\n",
    "    analyzer = count_vectorizer.build_analyzer()\n",
    "    \n",
    "    # Map the token to its indices and each index to token\n",
    "    analyzed_text = analyzer(text_instance[0])\n",
    "    dict_token_idxes = defaultdict(list)\n",
    "    for (idx, key) in enumerate(analyzed_text):\n",
    "        dict_token_idxes[key].append(idx)\n",
    "        \n",
    "    # Now create the samples\n",
    "    bin_samples = np.ones((num_samples, word_dim))\n",
    "    \n",
    "    # For each sample, choose how many features we will keep\n",
    "    num_words_keep = np.random.randint(1, word_dim, size=num_samples)\n",
    "    \n",
    "    # First row is original sample, hence we keep all words\n",
    "    num_words_keep[0] = word_dim\n",
    "    \n",
    "    # Sample binary data\n",
    "    bin_samples = np.array(list(map(lambda x: np.random.binomial(1, x/word_dim, size=word_dim), \n",
    "                                    num_words_keep))).astype(bool)\n",
    "    \n",
    "    # Now create synthetic raw text\n",
    "    features = np.array(count_vectorizer.get_feature_names())\n",
    "    \n",
    "    # boolean index the words\n",
    "    list_words = [features[row] for row in bin_samples]\n",
    "    \n",
    "    # Get the synthetic raw text\n",
    "    ### TODO: this is probably the most expensive part - can we optimize this?\n",
    "    def unravel_text(list_word):\n",
    "        a = []\n",
    "        for word in list_word:\n",
    "            a.extend(dict_token_idxes[word])\n",
    "        a = sorted(a)\n",
    "        return ' '.join(np.array(analyzed_text)[a])\n",
    "    \n",
    "    list_raw_synthetic = list(map(unravel_text, list_words))\n",
    "\n",
    "    # Get model predictions\n",
    "    model_pred = predict_fn(list_raw_synthetic)\n",
    "    \n",
    "    # Get distances between original sample and synthetic ones\n",
    "    distances = pairwise_distances(bin_samples[0].reshape((1, -1)), bin_samples, metric='cosine').ravel()\n",
    "    \n",
    "    # Get weights\n",
    "    def kernel_fn(distances, kernel_width):\n",
    "        return np.sqrt(np.exp(-(distances ** 2) / kernel_width ** 2))\n",
    "\n",
    "    weights = kernel_fn(distances, kernel_width=0.75 * np.sqrt(word_dim))\n",
    "    \n",
    "    # Solve\n",
    "    solver = Ridge(alpha=1, fit_intercept=True)\n",
    "    solver.fit(bin_samples, model_pred[:, label], sample_weight=weights)\n",
    "    \n",
    "    # Get explanation\n",
    "    importances = solver.coef_\n",
    "    explanations = sorted(list(zip(features, importances)), \n",
    "                          key=lambda x: x[1], reverse=True)[:num_features]\n",
    "    \n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mattingly', 0.20780741991599813),\n",
       " ('luriem', 0.07959609932238476),\n",
       " ('lurie', 0.07517863517897007),\n",
       " ('liberalizer', 0.07399448731148721),\n",
       " ('players', 0.0710768744329371),\n",
       " ('yankee', 0.06994912722979445),\n",
       " ('allegheny', 0.06675727458508018),\n",
       " ('alleg', 0.05867803849984341),\n",
       " ('game', 0.05825432891236671),\n",
       " ('career', 0.05472549515534699)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_instance(raw_test.data[0], predict_fn, label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mattingly', 0.15814561673299102),\n",
       " ('Liberalizer', 0.0472819598152857),\n",
       " ('luriem', 0.04433302535343226),\n",
       " ('Yankee', 0.04127184243710446),\n",
       " ('Allegheny', 0.0399793866876642),\n",
       " ('Lurie', 0.03991892553345141),\n",
       " ('PLAYERS', 0.03664036266282317),\n",
       " ('game', 0.03593865911584771),\n",
       " ('tesla', -0.05375952068502894),\n",
       " ('njit', -0.061585384929271174)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_text_explainer = LimeTextExplainer(\n",
    "    class_names=categories\n",
    ")\n",
    "\n",
    "exp = lime_text_explainer.explain_instance(\n",
    "    text_instance=raw_test.data[0],\n",
    "    classifier_fn=predict_fn,\n",
    "    labels=(0,)\n",
    ")\n",
    "\n",
    "list(sorted(exp.as_list(0), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386 ms ± 11 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit explain_instance(raw_test.data[0], predict_fn, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630 ms ± 13.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit exp = lime_text_explainer.explain_instance(text_instance=raw_test.data[0], classifier_fn=predict_fn, labels=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
